<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8"/>
    <title>Visual Relationship Detection</title>
    <!-- Keep this stylesheet first to not break .js code -->
    <link rel="stylesheet" type="text/css" media="screen" href="./style.css"/>
    <link href="https://fonts.googleapis.com/css?family=Raleway" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Lato&subset=latin,latin-ext" rel="stylesheet" type="text/css">
</head>
<body>
    <div id="content-container">
        <header>
            <div class="text-container">
                <h1>Visual Relationship Detection</h1>
                <h3><em>Paul Fang, Raymond Heberer, Jasper Jenkins, Abhinav Pelapudi</em></h3>
                <p>Finding spatial relations, or more generally visual relationship detection, is a classification task performed on image data. In order to do so, instances of objects must first be found within the image. Then, for each pair of detections, relationships - often called "predicates" - are predicted.</p>
                <p>In this capstone we study the academic literature and implement existing approaches to finding relations between objects in images. Researchers from Stanford University conclude in a 2016 study that "while this task is easy for humans, Visual Relationship Detection is still a hard unsolved task" (Lu & Krishna). By reproducing the work of researchers in this domain, we have obtained a more precise understanding of what challenges - both theoretical and practical - make this task so challenging.</p>
            </div>
        </header>
        <section>
            <hr>
            <article id="CNN-article">
                <div class="text-container">
                    <h2>Background</h2>
                    <p>Learning spatial relationships is a recent problem, enabled by advances in object detection and localization. Early research posits that the image features acquired in training a model for the task can be used to improve other computer vision tasks, such as image segmentation (Gould et al. 2008), and scene understanding (Kumar & Kollar 2010). In recent years interest in the problem has grown, with "holistic scene understanding" becoming a goal in and of itself, and applications linking natural language with image processing (e.g. knowledge extraction, content retrieval) growing more prominent (Xiaoden et al. 2017).</p>
                    <p>To facilitate this research, various datasets have emerged containing different amounts of predefined object categories and relationships. While earlier papers would adapt subsets of existing object detection datasets such as PASCAL VOC, ImageNet, and COCO, the Visual Genome Dataset (VGD) was introduced in December 2015 as a comprehensively annotated dataset for holistic scene understanding - containing not only objects and relationships, but also attributes describing objects (Krishna et al.). Preceding the VGD was the Visual Relation Detection (VRD), annotated with 100 object categories and 70 predicates, including both spatial and nonspatial relationships.</p>
                    <p>Existing approaches rely on state-of-the-art object detection, such as Faster R-CNN, to provide input to the relation classification modules. In turn, Faster R-CNN and other object detection systems typically use convolutional architectures first designed for image classification (e.g. VGG16, ResNet, Inception). For both stages, transfer learning from models trained for an image classification task such as ImageNet is common practice. Sharing the weights between the corresponding convolutional layers of different modules (e.g. region proposal and object classification) is also done in some cases (Ren et al. 2015).</p>
                    <p>Early research only had access to datasets with a handful of defined and annotated relationships. Partly because of this, it was feasible to train separate detectors for each type of relation (Sadeghi & Farhadi 2011). However, with datasets such as the VRD this would not be practical. Therefore, the use of a single classifier has become more typical. In a similar fashion, whereas classifying relationships between all pairs of detected objects exhaustively has been the initial approach to the task, more principled approaches have been proposed, for instance the use of a reinforcement learning framework (Xiaoden et al. 2017).</p>
                    <p>Visual relationship detection remains a challenging task, with the relative shortage of data being a significant bottleneck, along with the infeasibility of comprehensively annotating scenes of sufficient complexity. Researchers address this latter problem by reporting recall, rather than mean absolute precision, as this metric does not punish "false" positives that may actually be in an image.</p>
                </div>
            </article>
            <hr>
            <article id="RL-article">
                <div class="text-container">
                    <h2>Approach</h2>
                    <p>A significant proportion of the capstone hours went towards investigating and implementing object detection systems. Notably, a Tensorflow implementation of Faster R-CNN was built, but was ultimately found to be unsuitable due to a number of inefficiencies and a lack of time to either correct them or train a model with them. Because of this, an existing Keras implementation of YOLOv3 was settled on and trained for the VRD dataset using transfer learning from pretrained weights.</p>
                    <p>Faster R-CNN extracts image features through a convolutional network architecture such as VGG16, which are then used as input to two modules: a region proposal network (RPN) and a object classification and localization network; for this second module the Fast R-CNN network architecture is employed. The regresses box coordinates from a set of predefined "anchors" and provides an overall confidence score of whether or not each region contains an object, while Fast R-CNN refines these coordinates to produce final bounding boxes along with a category-wise classification score. This involves significant amounts of preprocessing different parameterizations of box coordinates, which was done outside of the Tensorflow graph in our implementation. Doing so increased the computational time requirements significantly. As a result, training a detector for the VRD from scratch (without pretrained weights) was infeasible.</p>
                    <p>YOLOv3 is an example of a "single shot" object detector, which trades off accuracy for increased speed. The Keras implementation provided scripts for obtaining pretrained weights from the author of YOLO, P.J. Reddie. We chose to use the "tiny YOLO" version of the weights, which use the VGG16 architecture instead of ResNet101, wanting to minimize computational cost in order to obtain initial findings with the visual relations.</p>
                    <p>For classifying relationships, following Lu & Krishna's approach (2016), a lightweight "visual relationship module" was implemented that classifies relationships given features from two detected objects. This is essentially a deep feed-forward network that first uses region-of-interest pooling to obtain fixed-dimension features from sub-windows of an image feature map. These features are "fused" using 2048 unit hidden layers, and a softmax output layer predicts the type of relationship present.</p>
                    <p>The object detector and visual relationship module were trained in the cloud on a single Nvidia Tesla K80 GPU, with FloydHub as the provider. The pretrained YOLOv3 weights were frozen for 20 epochs while the newly initialized output layer was trained, before the full model was fine-tuned for another 20 epochs. In the first experiment, the visual relationship module was trained for 20 epochs, but this led to overfitting on the small VRD dataset; only 4000 images are present in the training partition.</p>
                    <p>In the second experiment, the module was trained for 5 epochs with a learning rate of 0.001, but was underfitted. At this point in training, the classifier has only learned to predict the most common type of relationship, and the confidence scores are very poorly tuned.</p>
                    <p>A Flask application is deployed on FloydHub's servers containing the current model. Given an image, it return JSON annotations in the style of the VRD dataset. A separate front-end package was also developed to parse these annotations and visualize the relationships detected in the scene as a directed graph.</p>
                </div>
            </article>
            <hr>
            <article id="GAN-article">
                <div class="text-container">
                    <h2>Conclusion & Next Steps</h2>

                    <p>Reproducing results from this recent computer vision task has proven to be very challenging for a number of reasons.</p>
                    <p>Most notably, the research assumes familiarity with state-of-the-art object detection, and provides very scarce implementation details for their object detector modules. Most likely, existing implementations are used, and our efforts in engineering a Faster R-CNN showed that reproducing the network architectures was not sufficient to obtain a practical object detector, but that many image processing and computer vision techniques are needed to get something production ready.</p>
                    <p>Also, with a small dataset, a deep neural network based classification model is prone to overfitting. It remains to be seen whether some middle ground between memorizing the data and making undertuned guesses can be found. It may be the case that the accuracy trade-off made by a single shot detector such as YOLOv3 will have a significant impact on the potential of a visual relationship classifier, and that the features extracted will be sufficient for fine-tuned classification.</p>
                    <p>To test this, the next step is to experiment with different sets of hyperparameters and training strategies to see what the potential of a YOLO-based classification module is on the VRD dataset. From there, a more accurate object detector such as Faster R-CNN can be substituted in, and frameworks for deciding on object instances to focus on experimented with.</p>
                </div>
            </article>
        </section>
    </div>
</body>
<script type="text/javascript" src="./bundle.js"></script>
</html>
